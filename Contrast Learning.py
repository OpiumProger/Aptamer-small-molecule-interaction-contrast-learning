import torch
import torch.nn as nn
import torch.nn.functional as F
import numpy as np
import pandas as pd
from torch.utils.data import Dataset, DataLoader, random_split
import matplotlib.pyplot as plt
from tqdm import tqdm
import os
import warnings
from sklearn.metrics import roc_curve, auc, roc_auc_score
from sklearn.manifold import TSNE
import umap.umap_ as umap
from sklearn.decomposition import PCA
from sklearn.preprocessing import StandardScaler
import seaborn as sns

warnings.filterwarnings('ignore')

# –î–æ–±–∞–≤–ª—è–µ–º –¥–ª—è Windows multiprocessing
if __name__ == '__main__':
    print("=" * 60)
    print("INFO-NCE –° –£–õ–£–ß–®–ï–ù–ò–Ø–ú–ò –î–õ–Ø –ö–õ–ê–°–¢–ï–†–ò–ó–ê–¶–ò–ò")
    print("=" * 60)

    # ==================== 1. –ó–ê–ì–†–£–ó–ö–ê –ï–î–ò–ù–û–ì–û –§–ê–ô–õ–ê ====================
    print("\nüì• –ó–∞–≥—Ä—É–∑–∫–∞ –µ–¥–∏–Ω–æ–≥–æ —Ñ–∞–π–ª–∞ —Å —ç–º–±–µ–¥–¥–∏–Ω–≥–∞–º–∏...")


    def load_unified_embeddings(file_path):
        """
        –ó–∞–≥—Ä—É–∂–∞–µ—Ç –µ–¥–∏–Ω—ã–π —Ñ–∞–π–ª —Å —ç–º–±–µ–¥–¥–∏–Ω–≥–∞–º–∏ sequence –∏ SMILES
        –∏ —Ä–∞–∑–¥–µ–ª—è–µ—Ç –∏—Ö –Ω–∞ positive –∏ negative
        """
        print(f"  –ó–∞–≥—Ä—É–∑–∫–∞: {os.path.basename(file_path)}")

        try:
            df = pd.read_csv(file_path)
        except Exception as e:
            print(f"–û—à–∏–±–∫–∞ –∑–∞–≥—Ä—É–∑–∫–∏ —Ñ–∞–π–ª–∞: {e}")
            return None

        print(f"‚úÖ –§–∞–π–ª –∑–∞–≥—Ä—É–∂–µ–Ω. –†–∞–∑–º–µ—Ä: {df.shape}")
        print(f"üìã –ö–æ–ª–æ–Ω–∫–∏: {list(df.columns)}")

        # –ù–∞—Ö–æ–¥–∏–º –∫–æ–ª–æ–Ω–∫–∏ —Å —ç–º–±–µ–¥–¥–∏–Ω–≥–∞–º–∏
        seq_emb_cols = [col for col in df.columns if col.startswith('seq_emb_')]
        smi_emb_cols = [col for col in df.columns if col.startswith('smi_emb_')]

        if not seq_emb_cols or not smi_emb_cols:
            print("–ù–µ –Ω–∞–π–¥–µ–Ω—ã –∫–æ–ª–æ–Ω–∫–∏ —Å —ç–º–±–µ–¥–¥–∏–Ω–≥–∞–º–∏ seq_emb_* –∏–ª–∏ smi_emb_*")
            print("   –ü—Ä–æ–≤–µ—Ä—å—Ç–µ, —á—Ç–æ —Ñ–∞–π–ª —Å–æ–¥–µ—Ä–∂–∏—Ç –∫–æ–ª–æ–Ω–∫–∏ seq_emb_0... –∏ smi_emb_0...")
            return None

        print(f"–ù–∞–π–¥–µ–Ω–æ –∫–æ–ª–æ–Ω–æ–∫ seq_emb: {len(seq_emb_cols)}")
        print(f"–ù–∞–π–¥–µ–Ω–æ –∫–æ–ª–æ–Ω–æ–∫ smi_emb: {len(smi_emb_cols)}")

        # –ü—Ä–æ–≤–µ—Ä—è–µ–º –∫–æ–ª–æ–Ω–∫–∏ label
        if 'label' not in df.columns:
            print(" –ö–æ–ª–æ–Ω–∫–∞ 'label' –Ω–µ –Ω–∞–π–¥–µ–Ω–∞ –≤ —Ñ–∞–π–ª–µ")
            print(" –§–∞–π–ª –¥–æ–ª–∂–µ–Ω —Å–æ–¥–µ—Ä–∂–∞—Ç—å –∫–æ–ª–æ–Ω–∫—É 'label' —Å–æ –∑–Ω–∞—á–µ–Ω–∏—è–º–∏ 0 –∏–ª–∏ 1")
            return None

        # –ü—Ä–æ–≤–µ—Ä—è–µ–º –∑–Ω–∞—á–µ–Ω–∏—è label
        unique_labels = sorted(df['label'].unique())
        print(f"üìä –£–Ω–∏–∫–∞–ª—å–Ω—ã–µ –∑–Ω–∞—á–µ–Ω–∏—è label: {unique_labels}")

        # –†–∞–∑–¥–µ–ª—è–µ–º –Ω–∞ positive –∏ negative
        pos_mask = df['label'] == 1
        neg_mask = df['label'] == 0

        print(f"\ –†–∞–∑–¥–µ–ª–µ–Ω–∏–µ –ø–æ label:")
        print(f"   ‚Ä¢ Positive (label=1): {pos_mask.sum()} —Å—Ç—Ä–æ–∫")
        print(f"   ‚Ä¢ Negative (label=0): {neg_mask.sum()} —Å—Ç—Ä–æ–∫")

        if pos_mask.sum() == 0:
            print("‚ùå –ù–µ—Ç positive –¥–∞–Ω–Ω—ã—Ö (label=1)")
            return None

        if neg_mask.sum() == 0:
            print(" –ù–µ—Ç negative –¥–∞–Ω–Ω—ã—Ö (label=0)")
            print("–ë—É–¥–µ—Ç –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∞ —Ç–æ–ª—å–∫–æ positive —á–∞—Å—Ç—å")

        # –ò–∑–≤–ª–µ–∫–∞–µ–º —ç–º–±–µ–¥–¥–∏–Ω–≥–∏
        apt_pos_emb = df.loc[pos_mask, seq_emb_cols].values.astype(np.float32)
        smi_pos_emb = df.loc[pos_mask, smi_emb_cols].values.astype(np.float32)

        if neg_mask.sum() > 0:
            apt_neg_emb = df.loc[neg_mask, seq_emb_cols].values.astype(np.float32)
            smi_neg_emb = df.loc[neg_mask, smi_emb_cols].values.astype(np.float32)
        # else:
        #     # –ï—Å–ª–∏ –Ω–µ—Ç negative, —Å–æ–∑–¥–∞–µ–º –ø—É—Å—Ç—ã–µ –º–∞—Å—Å–∏–≤—ã
        #     apt_neg_emb = np.array([], dtype=np.float32).reshape(0, len(seq_emb_cols))
        #     smi_neg_emb = np.array([], dtype=np.float32).reshape(0, len(smi_emb_cols))

        print(f"\n–≠–º–±–µ–¥–¥–∏–Ω–≥–∏ –∏–∑–≤–ª–µ—á–µ–Ω—ã:")
        print(f"   ‚Ä¢ Positive –∞–ø—Ç–∞–º–µ—Ä—ã: {apt_pos_emb.shape}")
        print(f"   ‚Ä¢ Positive SMILES: {smi_pos_emb.shape}")
        print(f"   ‚Ä¢ Negative –∞–ø—Ç–∞–º–µ—Ä—ã: {apt_neg_emb.shape}")
        print(f"   ‚Ä¢ Negative SMILES: {smi_neg_emb.shape}")

        return {
            'apt_pos': apt_pos_emb,
            'smi_pos': smi_pos_emb,
            'apt_neg': apt_neg_emb,
            'smi_neg': smi_neg_emb,
            'df': df,
            'seq_dim': len(seq_emb_cols),
            'smi_dim': len(smi_emb_cols)
        }


    # –ó–∞–≥—Ä—É–∂–∞–µ–º —Ñ–∞–π–ª (—É–∫–∞–∂–∏—Ç–µ –ø—É—Ç—å –∫ –≤–∞—à–µ–º—É —Ñ–∞–π–ª—É)
    input_file = "AptaBench_dataset_v2_with_embeddings.csv"  # –ò–ó–ú–ï–ù–ò–¢–ï –ù–ê –í–ê–® –§–ê–ô–õ

    # –ó–∞–≥—Ä—É–∂–∞–µ–º –¥–∞–Ω–Ω—ã–µ
    data = load_unified_embeddings(input_file)

    if data is None:
        print("–ù–µ —É–¥–∞–ª–æ—Å—å –∑–∞–≥—Ä—É–∑–∏—Ç—å –¥–∞–Ω–Ω—ã–µ")
        exit()

    # –ò–∑–≤–ª–µ–∫–∞–µ–º –¥–∞–Ω–Ω—ã–µ
    apt_pos_emb = data['apt_pos']
    smi_pos_emb = data['smi_pos']
    apt_neg_emb = data['apt_neg']
    smi_neg_emb = data['smi_neg']
    seq_dim = data['seq_dim']
    smi_dim = data['smi_dim']

    print(f"\n–î–∞–Ω–Ω—ã–µ –ø–æ–¥–≥–æ—Ç–æ–≤–ª–µ–Ω—ã:")
    print(f"   ‚Ä¢ Positive –ø–∞—Ä—ã: {apt_pos_emb.shape[0]}")
    print(f"   ‚Ä¢ Negative –ø–∞—Ä—ã: {apt_neg_emb.shape[0]}")
    print(f"   ‚Ä¢ –†–∞–∑–º–µ—Ä–Ω–æ—Å—Ç—å sequence: {seq_dim}")
    print(f"   ‚Ä¢ –†–∞–∑–º–µ—Ä–Ω–æ—Å—Ç—å SMILES: {smi_dim}")

    # ==================== 2. –ü–†–û–°–¢–û–ô –î–ê–¢–ê–°–ï–¢ ====================
    print("\n–°–æ–∑–¥–∞–Ω–∏–µ –¥–∞—Ç–∞—Å–µ—Ç–∞...")


    class SimpleContrastiveDataset(Dataset):
        """–ü—Ä–æ—Å—Ç–æ–π –¥–∞—Ç–∞—Å–µ—Ç —Å positive –∏ negative –ø–∞—Ä–∞–º–∏"""

        def __init__(self, apt_pos, smi_pos, apt_neg, smi_neg):
            # Positive –ø–∞—Ä—ã
            n_pos = min(len(apt_pos), len(smi_pos))
            self.apt_pos = torch.FloatTensor(apt_pos[:n_pos])
            self.smi_pos = torch.FloatTensor(smi_pos[:n_pos])

            # Negative –ø–∞—Ä—ã
            n_neg = min(len(apt_neg), len(smi_neg))
            self.apt_neg = torch.FloatTensor(apt_neg[:n_neg])
            self.smi_neg = torch.FloatTensor(smi_neg[:n_neg])

            # –°–æ–∑–¥–∞–µ–º –ø–∞—Ä—ã
            self.pairs = []
            self.labels = []

            # Positive –ø–∞—Ä—ã
            for i in range(n_pos):
                self.pairs.append(('pos', i))
                self.labels.append(1.0)

            # Negative –ø–∞—Ä—ã (—Å—Ç–æ–ª—å–∫–æ –∂–µ —Å–∫–æ–ª—å–∫–æ positive, –µ—Å–ª–∏ negative –¥–æ—Å—Ç–∞—Ç–æ—á–Ω–æ)
            n_neg_to_use = min(n_pos, n_neg)  # –ë–µ—Ä–µ–º –º–∏–Ω–∏–º—É–º
            for i in range(n_neg_to_use):
                self.pairs.append(('neg', i))
                self.labels.append(0.0)

            print(f"   –°–æ–∑–¥–∞–Ω–æ –ø–∞—Ä: {len(self.pairs)}")
            print(f"     ‚Ä¢ Positive: {n_pos}")
            print(f"     ‚Ä¢ Negative: {n_neg_to_use}")

            self.n_pos = n_pos
            self.n_neg = n_neg_to_use

        def __len__(self):
            return len(self.pairs)

        def __getitem__(self, idx):
            pair_type, pair_idx = self.pairs[idx]

            if pair_type == 'pos':
                apt_emb = self.apt_pos[pair_idx]
                smi_emb = self.smi_pos[pair_idx]
                label = 1.0
            else:  # 'neg'
                apt_emb = self.apt_neg[pair_idx]
                smi_emb = self.smi_neg[pair_idx]
                label = 0.0

            return apt_emb, smi_emb, torch.tensor(label, dtype=torch.float)

        def get_stats(self):
            """–í–æ–∑–≤—Ä–∞—â–∞–µ—Ç —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫—É –¥–∞—Ç–∞—Å–µ—Ç–∞"""
            return {
                'total': len(self),
                'positive': self.n_pos,
                'negative': self.n_neg,
                'pos_ratio': self.n_pos / len(self) if len(self) > 0 else 0
            }


    # –°–æ–∑–¥–∞–µ–º –¥–∞—Ç–∞—Å–µ—Ç
    dataset = SimpleContrastiveDataset(
        apt_pos_emb, smi_pos_emb,
        apt_neg_emb, smi_neg_emb
    )

    stats = dataset.get_stats()
    print(f"    –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞ –¥–∞—Ç–∞—Å–µ—Ç–∞:")
    print(f"   ‚Ä¢ –í—Å–µ–≥–æ –ø–∞—Ä: {stats['total']}")
    print(f"   ‚Ä¢ Positive: {stats['positive']} ({stats['pos_ratio']:.1%})")
    print(f"   ‚Ä¢ Negative: {stats['negative']} ({1 - stats['pos_ratio']:.1%})")

    # –†–∞–∑–¥–µ–ª—è–µ–º –Ω–∞ train/test
    train_size = int(0.8 * len(dataset))
    val_size = len(dataset) - train_size

    if train_size == 0 or val_size == 0:
        print(" –°–ª–∏—à–∫–æ–º –º–∞–ª–æ –¥–∞–Ω–Ω—ã—Ö –¥–ª—è —Ä–∞–∑–¥–µ–ª–µ–Ω–∏—è")
        print(" –ù—É–∂–Ω–æ –±–æ–ª—å—à–µ –¥–∞–Ω–Ω—ã—Ö –∏–ª–∏ –∏—Å–ø–æ–ª—å–∑—É–π—Ç–µ –≤–µ—Å—å –¥–∞—Ç–∞—Å–µ—Ç –¥–ª—è –æ–±—É—á–µ–Ω–∏—è")
        train_dataset = dataset
        val_dataset = dataset
    else:
        train_dataset, val_dataset = random_split(dataset, [train_size, val_size])

    # –£–±–∏—Ä–∞–µ–º num_workers –¥–ª—è Windows
    train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True, num_workers=0)
    val_loader = DataLoader(val_dataset, batch_size=32, shuffle=False, num_workers=0)

    print(f"üìä –†–∞–∑–º–µ—Ä—ã:")
    print(f"   ‚Ä¢ Train: {len(train_dataset)}")
    print(f"   ‚Ä¢ Val: {len(val_dataset)}")

    # ==================== 3. –£–õ–£–ß–®–ï–ù–ù–ê–Ø –ú–û–î–ï–õ–¨ ====================
    print("\n   –°–æ–∑–¥–∞–Ω–∏–µ —É–ª—É—á—à–µ–Ω–Ω–æ–π –º–æ–¥–µ–ª–∏...")


    class ImprovedProjector(nn.Module):
        def __init__(self, input_dim, output_dim=128):
            super().__init__()
            self.network = nn.Sequential(
                nn.Linear(input_dim, 512),
                nn.BatchNorm1d(512),
                nn.GELU(),
                nn.Dropout(0.2),
                nn.Linear(512, 256),
                nn.BatchNorm1d(256),
                nn.SiLU(),
                nn.Dropout(0.15),
                nn.Linear(256, output_dim),
            )

        def forward(self, x):
            return F.normalize(self.network(x), p=2, dim=1)  # L2 –Ω–æ—Ä–º–∞–ª–∏–∑–∞—Ü–∏—è


    class ContrastiveModel(nn.Module):
        def __init__(self, sequence_dim, smiles_dim):
            super().__init__()
            self.sequence_proj = ImprovedProjector(sequence_dim)
            self.smiles_proj = ImprovedProjector(smiles_dim)

        def forward(self, sequence_emb, smiles_emb):
            z_seq = self.sequence_proj(sequence_emb)
            z_smi = self.smiles_proj(smiles_emb)

            return z_seq, z_smi


    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
    print(f"    –ú–æ–¥–µ–ª—å —Å–æ–∑–¥–∞–Ω–∞ –Ω–∞ {device}")

    model = ContrastiveModel(seq_dim, smi_dim).to(device)
    print(f"   –ü–∞—Ä–∞–º–µ—Ç—Ä–æ–≤: {sum(p.numel() for p in model.parameters()):,}")

    # ==================== 4.–§–£–ù–ö–¶–ò–Ø –ü–û–¢–ï–†–¨ ====================
    print("\n   –ù–∞—Å—Ç—Ä–æ–π–∫–∞ –æ–±—É—á–µ–Ω–∏—è...")
    class SimpleContrastiveLoss(nn.Module):
        def __init__(self, pos_threshold=0.6, neg_threshold=0.0, neg_weight=4.0):
            super().__init__()
            self.pos_threshold = pos_threshold  # Positive –¥–æ–ª–∂–Ω—ã –±—ã—Ç—å > —ç—Ç–æ–≥–æ
            self.neg_threshold = neg_threshold  # Negative –¥–æ–ª–∂–Ω—ã –±—ã—Ç—å < —ç—Ç–æ–≥–æ
            self.neg_weight = neg_weight  # –ù–∞—Å–∫–æ–ª—å–∫–æ negative –≤–∞–∂–Ω–µ–µ

            print(f"    SimpleContrastiveLoss:")
            print(f"   ‚Ä¢ Positive > {pos_threshold}")
            print(f"   ‚Ä¢ Negative < {neg_threshold}")
            print(f"   ‚Ä¢ Negative weight: {neg_weight}")

        def forward(self, z_seq, z_smi, labels):
            cos_sim = F.cosine_similarity(z_seq, z_smi, dim=-1)

            # Positive: —à—Ç—Ä–∞—Ñ –µ—Å–ª–∏ < threshold
            pos_loss = F.relu(self.pos_threshold - cos_sim) * labels

            # Negative: —à—Ç—Ä–∞—Ñ –µ—Å–ª–∏ > threshold
            neg_loss = F.relu(cos_sim - self.neg_threshold) * (1 - labels)

            return pos_loss.mean() + self.neg_weight * neg_loss.mean()


    # –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è:
    loss_fn = SimpleContrastiveLoss(
        pos_threshold=0.6,
        neg_threshold=0.0,
        neg_weight=5.0
    )
    optimizer = torch.optim.Adam(model.parameters(), lr=1e-4, weight_decay=1e-4)

    if len(dataset) > 100:
        scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=10, gamma=0.5)
    else:
        scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=5, gamma=0.7)


    print(f"   Positive threshold: {loss_fn.pos_threshold}")
    print(f"   Negative threshold: {loss_fn.neg_threshold}")
    print(f"   Negative weight: {loss_fn.neg_weight}")
    print(f"   Learning rate: {optimizer.param_groups[0]['lr']}")
    print(f"   Weight decay: {optimizer.param_groups[0]['weight_decay']}")

    # ==================== 5. –û–ë–£–ß–ï–ù–ò–ï ====================
    print("\n   –ù–∞—á–∏–Ω–∞–µ–º –æ–±—É—á–µ–Ω–∏–µ...")

    # –ê–¥–∞–ø—Ç–∏–≤–Ω–æ–µ –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ —ç–ø–æ—Ö
    if len(dataset) < 100:
        epochs = 20
    elif len(dataset) < 1000:
        epochs = 30
    else:
        epochs = 40

    print(f"    –ö–æ–ª–∏—á–µ—Å—Ç–≤–æ —ç–ø–æ—Ö: {epochs}")

    history = {
        'train_loss': [], 'val_loss': [],
        'train_separation': [], 'val_separation': [],
        'train_auc': [], 'val_auc': [],
        'train_pos_mean': [], 'train_neg_mean': [],
        'val_pos_mean': [], 'val_neg_mean': []
    }

    best_val_auc = 0.0
    best_val_separation = 0.0
    patience = 5
    patience_counter = 0

    for epoch in range(epochs):
        model.train()
        train_loss = 0
        train_pos_sim = []
        train_neg_sim = []
        train_preds = []
        train_labels = []

        if len(train_loader) > 0:
            pbar = tqdm(train_loader, desc=f"Epoch {epoch + 1}/{epochs} [Train]", leave=False)
            for seq_batch, smi_batch, labels in pbar:
                seq_batch = seq_batch.to(device)
                smi_batch = smi_batch.to(device)
                labels = labels.to(device)

                # –ü—Ä—è–º–æ–π –ø—Ä–æ—Ö–æ–¥
                z_seq, z_smi = model(seq_batch, smi_batch)

                # –í—ã—á–∏—Å–ª—è–µ–º loss
                loss = loss_fn(z_seq, z_smi, labels)

                # –í—ã—á–∏—Å–ª—è–µ–º —Å—Ö–æ–¥—Å—Ç–≤–∞ –¥–ª—è —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∏
                with torch.no_grad():
                    cos_sim = F.cosine_similarity(z_seq, z_smi, dim=-1)

                    # –°–æ—Ö—Ä–∞–Ω—è–µ–º –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏—è –∏ –º–µ—Ç–∫–∏ –¥–ª—è AUC
                    train_preds.extend(cos_sim.cpu().numpy())
                    train_labels.extend(labels.cpu().numpy())

                    pos_mask = labels == 1
                    neg_mask = labels == 0

                    if pos_mask.sum() > 0:
                        train_pos_sim.extend(cos_sim[pos_mask].cpu().numpy())
                    if neg_mask.sum() > 0:
                        train_neg_sim.extend(cos_sim[neg_mask].cpu().numpy())

                # –û–ø—Ç–∏–º–∏–∑–∞—Ü–∏—è
                optimizer.zero_grad()
                loss.backward()
                torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)
                optimizer.step()

                train_loss += loss.item()
                pbar.set_postfix({'loss': f"{loss.item():.4f}"})

            avg_train_loss = train_loss / len(train_loader) if len(train_loader) > 0 else 0
            train_separation = np.mean(train_pos_sim) - np.mean(train_neg_sim) if train_pos_sim and train_neg_sim else 0
            train_pos_mean = np.mean(train_pos_sim) if train_pos_sim else 0
            train_neg_mean = np.mean(train_neg_sim) if train_neg_sim else 0

            # –í—ã—á–∏—Å–ª—è–µ–º AUC –¥–ª—è train
            if len(train_preds) > 0 and len(set(train_labels)) > 1:
                train_auc = roc_auc_score(train_labels, train_preds)
            else:
                train_auc = 0.5
        else:
            avg_train_loss = 0
            train_separation = 0
            train_auc = 0.5
            train_pos_mean = 0
            train_neg_mean = 0

        # ===== VALIDATION =====
        model.eval()
        val_loss = 0
        val_pos_sim = []
        val_neg_sim = []
        val_preds = []
        val_labels = []

        with torch.no_grad():
            for seq_batch, smi_batch, labels in val_loader:
                seq_batch = seq_batch.to(device)
                smi_batch = smi_batch.to(device)
                labels = labels.to(device)

                z_seq, z_smi = model(seq_batch, smi_batch)
                loss = loss_fn(z_seq, z_smi, labels)
                val_loss += loss.item()

                cos_sim = F.cosine_similarity(z_seq, z_smi, dim=-1)

                # –°–æ—Ö—Ä–∞–Ω—è–µ–º –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏—è –∏ –º–µ—Ç–∫–∏ –¥–ª—è AUC
                val_preds.extend(cos_sim.cpu().numpy())
                val_labels.extend(labels.cpu().numpy())

                pos_mask = labels == 1
                neg_mask = labels == 0

                if pos_mask.sum() > 0:
                    val_pos_sim.extend(cos_sim[pos_mask].cpu().numpy())
                if neg_mask.sum() > 0:
                    val_neg_sim.extend(cos_sim[neg_mask].cpu().numpy())

        avg_val_loss = val_loss / len(val_loader) if len(val_loader) > 0 else 0
        val_separation = np.mean(val_pos_sim) - np.mean(val_neg_sim) if val_pos_sim and val_neg_sim else 0
        val_pos_mean = np.mean(val_pos_sim) if val_pos_sim else 0
        val_neg_mean = np.mean(val_neg_sim) if val_neg_sim else 0

        # –í—ã—á–∏—Å–ª—è–µ–º AUC –¥–ª—è validation
        if len(val_preds) > 0 and len(set(val_labels)) > 1:
            val_auc = roc_auc_score(val_labels, val_preds)
        else:
            val_auc = 0.5

        # ===== –°–û–•–†–ê–ù–ï–ù–ò–ï –ò–°–¢–û–†–ò–ò =====
        history['train_loss'].append(avg_train_loss)
        history['val_loss'].append(avg_val_loss)
        history['train_separation'].append(train_separation)
        history['val_separation'].append(val_separation)
        history['train_auc'].append(train_auc)
        history['val_auc'].append(val_auc)
        history['train_pos_mean'].append(train_pos_mean)
        history['train_neg_mean'].append(train_neg_mean)
        history['val_pos_mean'].append(val_pos_mean)
        history['val_neg_mean'].append(val_neg_mean)

        # ===== –í–´–í–û–î –ò–ù–§–û–†–ú–ê–¶–ò–ò =====
        print(f"\nEpoch {epoch + 1:02d}/{epochs}")
        print(f"  Train Loss: {avg_train_loss:.4f} | Val Loss: {avg_val_loss:.4f}")
        print(f"  Train AUC: {train_auc:.4f} | Val AUC: {val_auc:.4f}")
        print(f"  Train Separation: {train_separation:.4f} | Val Separation: {val_separation:.4f}")
        print(f"  Train Pos Mean: {train_pos_mean:.4f} | Train Neg Mean: {train_neg_mean:.4f}")
        print(f"  Val Pos Mean: {val_pos_mean:.4f} | Val Neg Mean: {val_neg_mean:.4f}")

        # ===== –°–û–•–†–ê–ù–ï–ù–ò–ï –õ–£–ß–®–ï–ô –ú–û–î–ï–õ–ò =====
        # –°–æ—Ö—Ä–∞–Ω—è–µ–º –ø–æ AUC –∏ separation
        if val_auc > best_val_auc or (val_auc == best_val_auc and val_separation > best_val_separation):
            if val_auc > best_val_auc:
                best_val_auc = val_auc
            if val_separation > best_val_separation:
                best_val_separation = val_separation
            patience_counter = 0
            torch.save({
                'epoch': epoch,
                'model_state_dict': model.state_dict(),
                'optimizer_state_dict': optimizer.state_dict(),
                'val_loss': avg_val_loss,
                'val_auc': val_auc,
                'val_separation': val_separation,
                'val_pos_mean': val_pos_mean,
                'val_neg_mean': val_neg_mean,
                'history': history
            }, 'best_model_improved.pth')
            print(f"  ‚úì –°–æ—Ö—Ä–∞–Ω–µ–Ω–∞ –ª—É—á—à–∞—è –º–æ–¥–µ–ª—å (AUC: {val_auc:.4f}, Sep: {val_separation:.4f})")
        else:
            patience_counter += 1
            if patience_counter >= patience:
                print(f"  ‚èπÔ∏è  Early stopping –Ω–∞ —ç–ø–æ—Ö–µ {epoch + 1}")
                break

        scheduler.step()

    print("\n   –û–±—É—á–µ–Ω–∏–µ –∑–∞–≤–µ—Ä—à–µ–Ω–æ!")


    # ==================== DBSCAN –§–£–ù–ö–¶–ò–ò ====================

    def analyze_with_dbscan(embeddings, labels, eps=0.3, min_samples=5):
        """
        –ü—Ä–∏–º–µ–Ω—è–µ—Ç DBSCAN –∫ —ç–º–±–µ–¥–¥–∏–Ω–≥–∞–º –¥–ª—è –∞–Ω–∞–ª–∏–∑–∞ —Å—Ç—Ä—É–∫—Ç—É—Ä—ã
        """
        from sklearn.cluster import DBSCAN

        print(f"\n  DBSCAN –∞–Ω–∞–ª–∏–∑ (eps={eps}, min_samples={min_samples}):")

        # DBSCAN
        clustering = DBSCAN(eps=eps, min_samples=min_samples, metric='cosine')
        cluster_labels = clustering.fit_predict(embeddings)

        # –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞
        n_clusters = len(set(cluster_labels)) - (1 if -1 in cluster_labels else 0)
        n_noise = list(cluster_labels).count(-1)

        print(f"   ‚Ä¢ –ù–∞–π–¥–µ–Ω–æ –∫–ª–∞—Å—Ç–µ—Ä–æ–≤: {n_clusters}")
        print(f"   ‚Ä¢ –í—ã–±—Ä–æ—Å–æ–≤ (noise): {n_noise}")
        print(f"   ‚Ä¢ –í—Å–µ–≥–æ —Ç–æ—á–µ–∫: {len(embeddings)}")

        # –ê–Ω–∞–ª–∏–∑ –ø–æ –∫–ª–∞—Å—Å–∞–º
        unique_labels = np.unique(labels)

        for label_val in unique_labels:
            mask = labels == label_val
            class_points = embeddings[mask]
            class_clusters = cluster_labels[mask]

            n_class_clusters = len(set(class_clusters)) - (1 if -1 in class_clusters else 0)
            n_class_noise = list(class_clusters).count(-1)

            print(f"\n   –ö–ª–∞—Å—Å {'Positive' if label_val == 1 else 'Negative'}:")
            print(f"     ‚Ä¢ –ö–ª–∞—Å—Ç–µ—Ä–æ–≤: {n_class_clusters}")
            print(f"     ‚Ä¢ –í—ã–±—Ä–æ—Å–æ–≤: {n_class_noise}")
            print(f"     ‚Ä¢ –í –∫–ª–∞—Å—Ç–µ—Ä–∞—Ö: {len(class_points) - n_class_noise}")

        return cluster_labels, clustering


    def find_problematic_negatives_with_dbscan(model, apt_neg_emb, smi_neg_emb,
                                               threshold=0.5, eps=0.4):
        """
        –ù–∞—Ö–æ–¥–∏—Ç –ø—Ä–æ–±–ª–µ–º–Ω—ã–µ negative –ø–∞—Ä—ã —Å –ø–æ–º–æ—â—å—é DBSCAN
        """
        from sklearn.cluster import DBSCAN

        model.eval()

        with torch.no_grad():
            # –ü–æ–ª—É—á–∞–µ–º —ç–º–±–µ–¥–¥–∏–Ω–≥–∏
            apt_tensor = torch.FloatTensor(apt_neg_emb).to(device)
            smi_tensor = torch.FloatTensor(smi_neg_emb).to(device)

            z_apt, z_smi = model(apt_tensor, smi_tensor)

            # –í—ã—á–∏—Å–ª—è–µ–º similarities
            similarities = F.cosine_similarity(z_apt, z_smi, dim=-1).cpu().numpy()

            # –û–±—ä–µ–¥–∏–Ω—è–µ–º —ç–º–±–µ–¥–¥–∏–Ω–≥–∏ –¥–ª—è –∫–ª–∞—Å—Ç–µ—Ä–∏–∑–∞—Ü–∏–∏
            combined_emb = np.hstack([z_apt.cpu().numpy(), z_smi.cpu().numpy()])

            # 1. –ù–∞—Ö–æ–¥–∏–º high similarity negative
            high_sim_mask = similarities > threshold
            high_sim_indices = np.where(high_sim_mask)[0]

            print(f"\nüîç –ü—Ä–æ–±–ª–µ–º–Ω—ã–µ negative (similarity > {threshold}): {len(high_sim_indices)}")

            if len(high_sim_indices) == 0:
                return [], [], None

            # 2. DBSCAN –Ω–∞ –ø—Ä–æ–±–ª–µ–º–Ω—ã—Ö negative
            problem_embeddings = combined_emb[high_sim_indices]

            dbscan = DBSCAN(eps=eps, min_samples=2, metric='cosine')
            cluster_labels = dbscan.fit_predict(problem_embeddings)

            # 3. –ê–Ω–∞–ª–∏–∑ –∫–ª–∞—Å—Ç–µ—Ä–æ–≤
            n_clusters = len(set(cluster_labels)) - (1 if -1 in cluster_labels else 0)

            print(f"   DBSCAN –æ–±–Ω–∞—Ä—É–∂–∏–ª {n_clusters} –∫–ª–∞—Å—Ç–µ—Ä–∞ –ø—Ä–æ–±–ª–µ–º–Ω—ã—Ö negative")

            # –°–æ–±–∏—Ä–∞–µ–º –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é –ø–æ –∫–ª–∞—Å—Ç–µ—Ä–∞–º
            clusters_info = []
            for cluster_id in set(cluster_labels):
                if cluster_id == -1:
                    continue  # –ü—Ä–æ–ø—É—Å–∫–∞–µ–º —à—É–º

                cluster_indices = np.where(cluster_labels == cluster_id)[0]
                original_indices = high_sim_indices[cluster_indices]

                cluster_sims = similarities[original_indices]

                clusters_info.append({
                    'cluster_id': cluster_id,
                    'size': len(cluster_indices),
                    'indices': original_indices,
                    'mean_similarity': cluster_sims.mean(),
                    'max_similarity': cluster_sims.max(),
                    'min_similarity': cluster_sims.min()
                })

            # –°–æ—Ä—Ç–∏—Ä—É–µ–º –ø–æ —Ä–∞–∑–º–µ—Ä—É –∫–ª–∞—Å—Ç–µ—Ä–∞
            clusters_info.sort(key=lambda x: x['size'], reverse=True)

            for i, cluster in enumerate(clusters_info[:5]):  # –¢–æ–ø-5 –∫–ª–∞—Å—Ç–µ—Ä–æ–≤
                print(f"\n   –ö–ª–∞—Å—Ç–µ—Ä #{cluster['cluster_id']}:")
                print(f"     ‚Ä¢ –†–∞–∑–º–µ—Ä: {cluster['size']} –ø–∞—Ä")
                print(f"     ‚Ä¢ –°—Ä–µ–¥–Ω–µ–µ similarity: {cluster['mean_similarity']:.3f}")
                print(f"     ‚Ä¢ –ú–∞–∫—Å–∏–º–∞–ª—å–Ω–æ–µ: {cluster['max_similarity']:.3f}")
                print(f"     ‚Ä¢ –ú–∏–Ω–∏–º–∞–ª—å–Ω–æ–µ: {cluster['min_similarity']:.3f}")

            return high_sim_indices, clusters_info, dbscan




    # ==================== 6. –ì–†–ê–§–ò–ö –û–ë–£–ß–ï–ù–ò–Ø ====================
    print("\n   –ì—Ä–∞—Ñ–∏–∫ –æ–±—É—á–µ–Ω–∏—è...")

    fig, axes = plt.subplots(2, 3, figsize=(18, 10))

    # 1. Loss
    axes[0, 0].plot(history['train_loss'], label='Train Loss', marker='o', markersize=3, linewidth=2)
    axes[0, 0].plot(history['val_loss'], label='Val Loss', marker='s', markersize=3, linewidth=2)
    axes[0, 0].set_xlabel('–≠–ø–æ—Ö–∞')
    axes[0, 0].set_ylabel('Loss')
    axes[0, 0].set_title('Loss –≤–æ –≤—Ä–µ–º—è –æ–±—É—á–µ–Ω–∏—è')
    axes[0, 0].legend()
    axes[0, 0].grid(True, alpha=0.3)

    # 2. Separation
    axes[0, 1].plot(history['train_separation'], label='Train Separation', marker='o', markersize=3, linewidth=2)
    axes[0, 1].plot(history['val_separation'], label='Val Separation', marker='s', markersize=3, linewidth=2)
    axes[0, 1].set_xlabel('–≠–ø–æ—Ö–∞')
    axes[0, 1].set_ylabel('–†–∞–∑–¥–µ–ª–µ–Ω–∏–µ (Positive - Negative)')
    axes[0, 1].set_title('–†–∞–∑–¥–µ–ª–µ–Ω–∏–µ –∫–ª–∞—Å—Å–æ–≤ –≤–æ –≤—Ä–µ–º—è –æ–±—É—á–µ–Ω–∏—è')
    axes[0, 1].legend()
    axes[0, 1].grid(True, alpha=0.3)

    # 3. AUC
    axes[0, 2].plot(history['train_auc'], label='Train AUC', marker='o', markersize=3, linewidth=2)
    axes[0, 2].plot(history['val_auc'], label='Val AUC', marker='s', markersize=3, linewidth=2)
    axes[0, 2].set_xlabel('–≠–ø–æ—Ö–∞')
    axes[0, 2].set_ylabel('ROC-AUC')
    axes[0, 2].set_title('ROC-AUC –≤–æ –≤—Ä–µ–º—è –æ–±—É—á–µ–Ω–∏—è')
    axes[0, 2].legend()
    axes[0, 2].grid(True, alpha=0.3)

    # 4. Positive Mean
    axes[1, 0].plot(history['train_pos_mean'], label='Train Pos Mean', marker='o', markersize=3, linewidth=2,
                    color='green')
    axes[1, 0].plot(history['val_pos_mean'], label='Val Pos Mean', marker='s', markersize=3, linewidth=2,
                    color='darkgreen')
    axes[1, 0].set_xlabel('–≠–ø–æ—Ö–∞')
    axes[1, 0].set_ylabel('–°—Ä–µ–¥–Ω–µ–µ —Å—Ö–æ–¥—Å—Ç–≤–æ')
    axes[1, 0].set_title('Positive –ø–∞—Ä—ã - —Å—Ä–µ–¥–Ω–µ–µ —Å—Ö–æ–¥—Å—Ç–≤–æ')
    axes[1, 0].legend()
    axes[1, 0].grid(True, alpha=0.3)
    axes[1, 0].axhline(y=0, color='black', linestyle='--', alpha=0.3)

    # 5. Negative Mean
    axes[1, 1].plot(history['train_neg_mean'], label='Train Neg Mean', marker='o', markersize=3, linewidth=2,
                    color='red')
    axes[1, 1].plot(history['val_neg_mean'], label='Val Neg Mean', marker='s', markersize=3, linewidth=2,
                    color='darkred')
    axes[1, 1].set_xlabel('–≠–ø–æ—Ö–∞')
    axes[1, 1].set_ylabel('–°—Ä–µ–¥–Ω–µ–µ —Å—Ö–æ–¥—Å—Ç–≤–æ')
    axes[1, 1].set_title('Negative –ø–∞—Ä—ã - —Å—Ä–µ–¥–Ω–µ–µ —Å—Ö–æ–¥—Å—Ç–≤–æ')
    axes[1, 1].legend()
    axes[1, 1].grid(True, alpha=0.3)
    axes[1, 1].axhline(y=0, color='black', linestyle='--', alpha=0.3)

    # 6. Combined
    axes[1, 2].plot(history['train_pos_mean'], label='Train Pos', marker='o', markersize=3, linewidth=2, color='green')
    axes[1, 2].plot(history['train_neg_mean'], label='Train Neg', marker='o', markersize=3, linewidth=2, color='red')
    axes[1, 2].plot(history['val_pos_mean'], label='Val Pos', marker='s', markersize=3, linewidth=2, color='darkgreen',
                    linestyle='--')
    axes[1, 2].plot(history['val_neg_mean'], label='Val Neg', marker='s', markersize=3, linewidth=2, color='darkred',
                    linestyle='--')
    axes[1, 2].set_xlabel('–≠–ø–æ—Ö–∞')
    axes[1, 2].set_ylabel('–°—Ä–µ–¥–Ω–µ–µ —Å—Ö–æ–¥—Å—Ç–≤–æ')
    axes[1, 2].set_title('–°—Ä–∞–≤–Ω–µ–Ω–∏–µ Positive –∏ Negative')
    axes[1, 2].legend()
    axes[1, 2].grid(True, alpha=0.3)
    axes[1, 2].axhline(y=0, color='black', linestyle='--', alpha=0.3)

    plt.tight_layout()
    plt.savefig('training_curves_improved.png', dpi=150, bbox_inches='tight')
    plt.show()
    print(" –ì—Ä–∞—Ñ–∏–∫ —Å–æ—Ö—Ä–∞–Ω–µ–Ω: training_curves_improved.png")

    # ==================== 7. –°–û–•–†–ê–ù–ï–ù–ò–ï –ê–î–ê–ü–¢–ò–†–û–í–ê–ù–ù–´–• –≠–ú–ë–ï–î–î–ò–ù–ì–û–í ====================
    print("\n   –ü–æ–ª—É—á–µ–Ω–∏–µ –∞–¥–∞–ø—Ç–∏—Ä–æ–≤–∞–Ω–Ω—ã—Ö —ç–º–±–µ–¥–¥–∏–Ω–≥–æ–≤...")


    def get_adapted_embeddings(model, seq_emb, smi_emb, batch_size=32):
        model.eval()

        # –£–±–µ–¥–∏–º—Å—è, —á—Ç–æ —Ä–∞–∑–º–µ—Ä–Ω–æ—Å—Ç–∏ —Å–æ–≤–ø–∞–¥–∞—é—Ç
        n_samples = min(len(seq_emb), len(smi_emb))

        if n_samples == 0:
            return np.array([]), np.array([])

        adapted_seq = []
        adapted_smi = []

        with torch.no_grad():
            for i in range(0, n_samples, batch_size):
                end_idx = min(i + batch_size, n_samples)

                seq_batch = torch.FloatTensor(seq_emb[i:end_idx]).to(device)
                smi_batch = torch.FloatTensor(smi_emb[i:end_idx]).to(device)

                z_seq, z_smi = model(seq_batch, smi_batch)

                adapted_seq.append(z_seq.cpu().numpy())
                adapted_smi.append(z_smi.cpu().numpy())

        if adapted_seq:
            adapted_seq = np.vstack(adapted_seq)
            adapted_smi = np.vstack(adapted_smi)
            print(f"   –ü—Ä–µ–æ–±—Ä–∞–∑–æ–≤–∞–Ω–æ: {n_samples} –ø–∞—Ä")
        else:
            adapted_seq = np.array([])
            adapted_smi = np.array([])
            print(f"   –ù–µ—Ç –¥–∞–Ω–Ω—ã—Ö –¥–ª—è –ø—Ä–µ–æ–±—Ä–∞–∑–æ–≤–∞–Ω–∏—è")
        return adapted_seq, adapted_smi

    print(" –ó–∞–≥—Ä—É–∑–∫–∞ –ª—É—á—à–µ–π –º–æ–¥–µ–ª–∏...")
    try:
        checkpoint = torch.load('best_model_improved.pth', map_location=device)
        model.load_state_dict(checkpoint['model_state_dict'])
        print(f"    –ú–æ–¥–µ–ª—å —ç–ø–æ—Ö–∏ {checkpoint['epoch'] + 1} –∑–∞–≥—Ä—É–∂–µ–Ω–∞")
        print(f"   Val AUC: {checkpoint['val_auc']:.4f}")
        print(f"   Val Separation: {checkpoint['val_separation']:.4f}")
        print(f"   Val Pos Mean: {checkpoint['val_pos_mean']:.4f}")
        print(f"   Val Neg Mean: {checkpoint['val_neg_mean']:.4f}")
    except:
        print(" –ù–µ —É–¥–∞–ª–æ—Å—å –∑–∞–≥—Ä—É–∑–∏—Ç—å –º–æ–¥–µ–ª—å, –∏—Å–ø–æ–ª—å–∑—É–µ–º —Ç–µ–∫—É—â—É—é")

    # –ü–æ–ª—É—á–∞–µ–º –∞–¥–∞–ø—Ç–∏—Ä–æ–≤–∞–Ω–Ω—ã–µ —ç–º–±–µ–¥–¥–∏–Ω–≥–∏
    print("\nüîß –ê–¥–∞–ø—Ç–∞—Ü–∏—è —ç–º–±–µ–¥–¥–∏–Ω–≥–æ–≤...")

    if len(apt_pos_emb) > 0 and len(smi_pos_emb) > 0:
        print("  Positive –ø–∞—Ä—ã...")
        seq_pos_adapted, smi_pos_adapted = get_adapted_embeddings(model, apt_pos_emb, smi_pos_emb)

        if len(seq_pos_adapted) > 0:
            # –°–æ—Ö—Ä–∞–Ω—è–µ–º positive
            np.save('seq_pos_adapted_improved.npy', seq_pos_adapted)
            np.save('smi_pos_adapted_improved.npy', smi_pos_adapted)
            print(f"  –°–æ—Ö—Ä–∞–Ω–µ–Ω–æ: {seq_pos_adapted.shape}")

    if len(apt_neg_emb) > 0 and len(smi_neg_emb) > 0:
        print("  Negative –ø–∞—Ä—ã...")
        seq_neg_adapted, smi_neg_adapted = get_adapted_embeddings(model, apt_neg_emb, smi_neg_emb)

        if len(seq_neg_adapted) > 0:
            # –°–æ—Ö—Ä–∞–Ω—è–µ–º negative
            np.save('seq_neg_adapted_improved.npy', seq_neg_adapted)
            np.save('smi_neg_adapted_improved.npy', smi_neg_adapted)
            print(f"    –°–æ—Ö—Ä–∞–Ω–µ–Ω–æ: {seq_neg_adapted.shape}")

    print(f"\n  –ê–¥–∞–ø—Ç–∏—Ä–æ–≤–∞–Ω–Ω—ã–µ —ç–º–±–µ–¥–¥–∏–Ω–≥–∏ —Å–æ—Ö—Ä–∞–Ω–µ–Ω—ã:")
    saved_files = []
    for fname in ['seq_pos_adapted_improved.npy', 'smi_pos_adapted_improved.npy',
                  'seq_neg_adapted_improved.npy', 'smi_neg_adapted_improved.npy']:
        if os.path.exists(fname):
            data = np.load(fname, allow_pickle=True)
            if hasattr(data, 'shape'):
                print(f"   ‚Ä¢ {fname}: {data.shape}")
                saved_files.append(fname)

    if not saved_files:
        print("     –§–∞–π–ª—ã –Ω–µ —Å–æ–∑–¥–∞–Ω—ã (–≤–æ–∑–º–æ–∂–Ω–æ, –Ω–µ—Ç –¥–∞–Ω–Ω—ã—Ö)")

    # ==================== 8. –ê–ù–ê–õ–ò–ó –†–ï–ó–£–õ–¨–¢–ê–¢–û–í ====================
    print("\n" + "=" * 60)
    print("üìä –ê–ù–ê–õ–ò–ó –†–ï–ó–£–õ–¨–¢–ê–¢–û–í")
    print("=" * 60)


    def compute_similarities(seq_emb, smi_emb):
        """–í—ã—á–∏—Å–ª—è–µ—Ç –∫–æ—Å–∏–Ω—É—Å–Ω—ã–µ —Å—Ö–æ–¥—Å—Ç–≤–∞ –º–µ–∂–¥—É –ø–∞—Ä–∞–º–∏"""
        if len(seq_emb) == 0 or len(smi_emb) == 0:
            return np.array([])

        similarities = []
        batch_size = 64

        with torch.no_grad():
            for i in range(0, len(seq_emb), batch_size):
                end_idx = min(i + batch_size, len(seq_emb))

                seq_batch = torch.FloatTensor(seq_emb[i:end_idx])
                smi_batch = torch.FloatTensor(smi_emb[i:end_idx])

                cos_sim = F.cosine_similarity(seq_batch, smi_batch, dim=-1)
                similarities.extend(cos_sim.numpy())

        return np.array(similarities)


    # –í—ã—á–∏—Å–ª—è–µ–º —Å—Ö–æ–¥—Å—Ç–≤–∞ –µ—Å–ª–∏ –µ—Å—Ç—å –∞–¥–∞–ø—Ç–∏—Ä–æ–≤–∞–Ω–Ω—ã–µ —ç–º–±–µ–¥–¥–∏–Ω–≥–∏
    if os.path.exists('seq_pos_adapted_improved.npy') and os.path.exists('smi_pos_adapted_improved.npy'):
        print("\n   –í—ã—á–∏—Å–ª–µ–Ω–∏–µ –∫–æ—Å–∏–Ω—É—Å–Ω—ã—Ö —Å—Ö–æ–¥—Å—Ç–≤...")

        seq_pos_adapted = np.load('seq_pos_adapted_improved.npy')
        smi_pos_adapted = np.load('smi_pos_adapted_improved.npy')

        pos_similarities = compute_similarities(seq_pos_adapted, smi_pos_adapted)

        if len(pos_similarities) > 0:
            print(f"\n  POSITIVE –ø–∞—Ä—ã:")
            print(f"   ‚Ä¢ –ö–æ–ª–∏—á–µ—Å—Ç–≤–æ: {len(pos_similarities)}")
            print(f"   ‚Ä¢ –°—Ä–µ–¥–Ω–µ–µ: {pos_similarities.mean():.4f}")
            print(f"   ‚Ä¢ –ú–µ–¥–∏–∞–Ω–∞: {np.median(pos_similarities):.4f}")
            print(f"   ‚Ä¢ Min-Max: {pos_similarities.min():.4f} - {pos_similarities.max():.4f}")

        if os.path.exists('seq_neg_adapted_improved.npy') and os.path.exists('smi_neg_adapted_improved.npy'):
            seq_neg_adapted = np.load('seq_neg_adapted_improved.npy')
            smi_neg_adapted = np.load('smi_neg_adapted_improved.npy')

            neg_similarities = compute_similarities(seq_neg_adapted, smi_neg_adapted)

            if len(neg_similarities) > 0:
                print(f"\n  NEGATIVE –ø–∞—Ä—ã:")
                print(f"   ‚Ä¢ –ö–æ–ª–∏—á–µ—Å—Ç–≤–æ: {len(neg_similarities)}")
                print(f"   ‚Ä¢ –°—Ä–µ–¥–Ω–µ–µ: {neg_similarities.mean():.4f}")
                print(f"   ‚Ä¢ –ú–µ–¥–∏–∞–Ω–∞: {np.median(neg_similarities):.4f}")
                print(f"   ‚Ä¢ Min-Max: {neg_similarities.min():.4f} - {neg_similarities.max():.4f}")

            # –†–∞–∑–¥–µ–ª–µ–Ω–∏–µ –µ—Å–ª–∏ –µ—Å—Ç—å –æ–±–∞ —Ç–∏–ø–∞
            if len(pos_similarities) > 0 and len(neg_similarities) > 0:
                separation = pos_similarities.mean() - neg_similarities.mean()
                print(f"\n  –†–ê–ó–î–ï–õ–ï–ù–ò–ï (separation): {separation:.4f}")

                # ==================== ROC-AUC –ö–†–ò–í–ê–Ø ====================
                print("\n   –ü–æ—Å—Ç—Ä–æ–µ–Ω–∏–µ ROC-AUC –∫—Ä–∏–≤–æ–π...")

                # –û–±—ä–µ–¥–∏–Ω—è–µ–º –≤—Å–µ –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏—è –∏ –º–µ—Ç–∫–∏
                all_preds = np.concatenate([pos_similarities, neg_similarities])
                all_labels = np.concatenate([np.ones_like(pos_similarities),
                                             np.zeros_like(neg_similarities)])

                # –í—ã—á–∏—Å–ª—è–µ–º ROC-AUC
                auc_score = roc_auc_score(all_labels, all_preds)
                print(f"   ROC-AUC Score: {auc_score:.4f}")

                # –í—ã—á–∏—Å–ª—è–µ–º ROC-–∫—Ä–∏–≤—É—é
                fpr, tpr, thresholds = roc_curve(all_labels, all_preds)

                # –í–∏–∑—É–∞–ª–∏–∑–∞—Ü–∏—è ROC-–∫—Ä–∏–≤–æ–π
                plt.figure(figsize=(10, 8))

                plt.subplot(2, 2, 1)
                plt.plot(fpr, tpr, color='darkorange', lw=2,
                         label=f'ROC curve (AUC = {auc_score:.3f})')
                plt.plot([0, 1], [0, 1], color='navy', lw=2, linestyle='--', label='Random')
                plt.xlim([0.0, 1.0])
                plt.ylim([0.0, 1.05])
                plt.xlabel('False Positive Rate')
                plt.ylabel('True Positive Rate')
                plt.title('ROC Curve')
                plt.legend(loc="lower right")
                plt.grid(True, alpha=0.3)

                # –í–∏–∑—É–∞–ª–∏–∑–∞—Ü–∏—è —Ä–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏—è —Å—Ö–æ–¥—Å—Ç–≤
                plt.subplot(2, 2, 2)
                plt.hist(pos_similarities, bins=50, alpha=0.6, color='green',
                         label=f'Positive (n={len(pos_similarities)}, Œº={pos_similarities.mean():.3f})',
                         density=True)
                plt.hist(neg_similarities, bins=50, alpha=0.6, color='red',
                         label=f'Negative (n={len(neg_similarities)}, Œº={neg_similarities.mean():.3f})',
                         density=True)

                plt.xlabel('–ö–æ—Å–∏–Ω—É—Å–Ω–æ–µ —Å—Ö–æ–¥—Å—Ç–≤–æ')
                plt.ylabel('–ü–ª–æ—Ç–Ω–æ—Å—Ç—å –≤–µ—Ä–æ—è—Ç–Ω–æ—Å—Ç–∏')
                plt.title(f'–†–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ —Å—Ö–æ–¥—Å—Ç–≤ (AUC: {auc_score:.3f})')
                plt.legend()
                plt.grid(True, alpha=0.3)

                # –î–æ–±–∞–≤–ª—è–µ–º –≤–µ—Ä—Ç–∏–∫–∞–ª—å–Ω—ã–µ –ª–∏–Ω–∏–∏ –¥–ª—è —Å—Ä–µ–¥–Ω–∏—Ö
                plt.axvline(x=pos_similarities.mean(), color='darkgreen', linestyle='--', linewidth=2)
                plt.axvline(x=neg_similarities.mean(), color='darkred', linestyle='--', linewidth=2)

                # ==================== –ö–û–ù–ö–ê–¢–ï–ù–ò–†–û–í–ê–ù–ù–´–ï –≠–ú–ë–ï–î–î–ò–ù–ì–ò ====================
                print("\n   –ü–æ–¥–≥–æ—Ç–æ–≤–∫–∞ –∫–æ–Ω–∫–∞—Ç–µ–Ω–∏—Ä–æ–≤–∞–Ω–Ω—ã—Ö —ç–º–±–µ–¥–¥–∏–Ω–≥–æ–≤ –¥–ª—è –≤–∏–∑—É–∞–ª–∏–∑–∞—Ü–∏–∏...")

                # –ö–æ–Ω–∫–∞—Ç–µ–Ω–∏—Ä—É–µ–º sequence –∏ smiles —ç–º–±–µ–¥–¥–∏–Ω–≥–∏
                pos_combined = np.hstack([seq_pos_adapted, smi_pos_adapted])
                neg_combined = np.hstack([seq_neg_adapted, smi_neg_adapted])

                # –û–±—ä–µ–¥–∏–Ω—è–µ–º –≤—Å–µ —ç–º–±–µ–¥–¥–∏–Ω–≥–∏
                all_embeddings = np.vstack([pos_combined, neg_combined])
                all_labels_combined = np.concatenate([np.ones(len(pos_combined)),
                                                      np.zeros(len(neg_combined))])

                # –ù–æ—Ä–º–∞–ª–∏–∑—É–µ–º –¥–∞–Ω–Ω—ã–µ
                scaler = StandardScaler()
                all_embeddings_scaled = scaler.fit_transform(all_embeddings)

                # –ü—Ä–æ–≤–µ—Ä—è–µ–º –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ –∫–æ–º–ø–æ–Ω–µ–Ω—Ç –¥–ª—è PCA
                n_components = min(50, all_embeddings_scaled.shape[1], all_embeddings_scaled.shape[0])

                # –ü—Ä–∏–º–µ–Ω—è–µ–º PCA –¥–ª—è —É–º–µ–Ω—å—à–µ–Ω–∏—è —Ä–∞–∑–º–µ—Ä–Ω–æ—Å—Ç–∏
                print(f"   –ü—Ä–∏–º–µ–Ω–µ–Ω–∏–µ PCA –¥–ª—è —É–º–µ–Ω—å—à–µ–Ω–∏—è —Ä–∞–∑–º–µ—Ä–Ω–æ—Å—Ç–∏ –¥–æ {n_components} –∫–æ–º–ø–æ–Ω–µ–Ω—Ç...")
                pca = PCA(n_components=n_components)
                embeddings_pca = pca.fit_transform(all_embeddings_scaled)

                print(f"   –û–±—ä—è—Å–Ω–µ–Ω–Ω–∞—è –¥–∏—Å–ø–µ—Ä—Å–∏—è PCA: {pca.explained_variance_ratio_.sum():.3f}")

                # ==================== t-SNE –í–ò–ó–£–ê–õ–ò–ó–ê–¶–ò–Ø ====================
                print("\n   –ü—Ä–∏–º–µ–Ω–µ–Ω–∏–µ t-SNE –¥–ª—è 2D –≤–∏–∑—É–∞–ª–∏–∑–∞—Ü–∏–∏...")

                # –ò—Å–ø–æ–ª—å–∑—É–µ–º —Ç–æ–ª—å–∫–æ –µ—Å–ª–∏ –¥–æ—Å—Ç–∞—Ç–æ—á–Ω–æ –¥–∞–Ω–Ω—ã—Ö
                if len(all_embeddings) > 10:
                    perplexity_value = min(30, len(all_embeddings) - 1)

                    try:
                        tsne = TSNE(n_components=2, perplexity=perplexity_value,
                                    random_state=42, n_iter=1000, verbose=0)
                        embeddings_tsne = tsne.fit_transform(embeddings_pca)

                        plt.subplot(2, 2, 3)

                        # –†–∞–∑–¥–µ–ª—è–µ–º —Ç–æ—á–∫–∏ –ø–æ –∫–ª–∞—Å—Å–∞–º
                        pos_indices = all_labels_combined == 1
                        neg_indices = all_labels_combined == 0

                        if pos_indices.sum() > 0:
                            plt.scatter(embeddings_tsne[pos_indices, 0],
                                        embeddings_tsne[pos_indices, 1],
                                        c='green', alpha=0.6, s=30,
                                        label=f'Positive ({pos_indices.sum()})',
                                        edgecolors='black', linewidth=0.5)

                        if neg_indices.sum() > 0:
                            plt.scatter(embeddings_tsne[neg_indices, 0],
                                        embeddings_tsne[neg_indices, 1],
                                        c='red', alpha=0.6, s=30,
                                        label=f'Negative ({neg_indices.sum()})',
                                        edgecolors='black', linewidth=0.5)

                        plt.xlabel('t-SNE 1')
                        plt.ylabel('t-SNE 2')
                        plt.title('t-SNE 2D –ø—Ä–æ–µ–∫—Ü–∏—è')
                        plt.legend()
                        plt.grid(True, alpha=0.3)

                    except Exception as e:
                        print(f"     –û—à–∏–±–∫–∞ t-SNE: {e}")

                # ==================== UMAP –í–ò–ó–£–ê–õ–ò–ó–ê–¶–ò–Ø ====================
                print("üìä –ü—Ä–∏–º–µ–Ω–µ–Ω–∏–µ UMAP –¥–ª—è 2D –≤–∏–∑—É–∞–ª–∏–∑–∞—Ü–∏–∏...")

                if len(all_embeddings) > 10:
                    try:
                        reducer = umap.UMAP(n_components=2, min_dist=0.3, random_state=42,
                                            n_neighbors=min(25, len(all_embeddings) - 1))
                        embeddings_umap = reducer.fit_transform(embeddings_pca)

                        plt.subplot(2, 2, 4)

                        # –†–∞–∑–¥–µ–ª—è–µ–º —Ç–æ—á–∫–∏ –ø–æ –∫–ª–∞—Å—Å–∞–º
                        if pos_indices.sum() > 0:
                            plt.scatter(embeddings_umap[pos_indices, 0],
                                        embeddings_umap[pos_indices, 1],
                                        c='green', alpha=0.6, s=30,
                                        label=f'Positive ({pos_indices.sum()})',
                                        edgecolors='black', linewidth=0.5)

                        if neg_indices.sum() > 0:
                            plt.scatter(embeddings_umap[neg_indices, 0],
                                        embeddings_umap[neg_indices, 1],
                                        c='red', alpha=0.6, s=30,
                                        label=f'Negative ({neg_indices.sum()})',
                                        edgecolors='black', linewidth=0.5)

                        plt.xlabel('UMAP 1')
                        plt.ylabel('UMAP 2')
                        plt.title('UMAP 2D –ø—Ä–æ–µ–∫—Ü–∏—è')
                        plt.legend()
                        plt.grid(True, alpha=0.3)

                    except Exception as e:
                        print(f"    –û—à–∏–±–∫–∞ UMAP: {e}")

                plt.tight_layout()
                plt.savefig('analysis_results_improved.png', dpi=150, bbox_inches='tight')
                plt.show()
                print("‚úì –ì—Ä–∞—Ñ–∏–∫ —Å–æ—Ö—Ä–∞–Ω–µ–Ω: analysis_results_improved.png")

    # ==================== DBSCAN –ê–ù–ê–õ–ò–ó ====================
    print("\n" + "=" * 60)
    print("üîç DBSCAN –ê–ù–ê–õ–ò–ó –ö–õ–ê–°–¢–ï–†–û–í")
    print("=" * 60)

    # 1. –ü–æ–ª—É—á–∞–µ–º –≤—Å–µ —ç–º–±–µ–¥–¥–∏–Ω–≥–∏
    all_embeddings = []
    all_labels = []

    with torch.no_grad():
        # Positive
        if len(apt_pos_emb) > 0:
            z_seq_pos, z_smi_pos = model(
                torch.FloatTensor(apt_pos_emb[:1000]).to(device),  # –û–≥—Ä–∞–Ω–∏—á–∏–º –¥–ª—è —Å–∫–æ—Ä–æ—Å—Ç–∏
                torch.FloatTensor(smi_pos_emb[:1000]).to(device)
            )
            combined_pos = torch.cat([z_seq_pos, z_smi_pos], dim=-1).cpu().numpy()
            all_embeddings.append(combined_pos)
            all_labels.extend([1] * len(combined_pos))

        # Negative
        if len(apt_neg_emb) > 0:
            z_seq_neg, z_smi_neg = model(
                torch.FloatTensor(apt_neg_emb[:1000]).to(device),
                torch.FloatTensor(smi_neg_emb[:1000]).to(device)
            )
            combined_neg = torch.cat([z_seq_neg, z_smi_neg], dim=-1).cpu().numpy()
            all_embeddings.append(combined_neg)
            all_labels.extend([0] * len(combined_neg))

    if all_embeddings:
        all_embeddings = np.vstack(all_embeddings)
        all_labels = np.array(all_labels)

        # –ó–∞–ø—É—Å–∫–∞–µ–º DBSCAN –∞–Ω–∞–ª–∏–∑
        cluster_labels, clustering = analyze_with_dbscan(
            all_embeddings, all_labels, eps=0.4, min_samples=5
        )

        # –í–∏–∑—É–∞–ª–∏–∑–∞—Ü–∏—è
        if len(all_embeddings) > 10:
            from sklearn.manifold import TSNE

            # t-SNE –¥–ª—è –≤–∏–∑—É–∞–ª–∏–∑–∞—Ü–∏–∏
            tsne = TSNE(n_components=2, perplexity=30, random_state=42)
            embeddings_2d = tsne.fit_transform(all_embeddings)

            plt.figure(figsize=(12, 10))

            # –¶–≤–µ—Ç –ø–æ DBSCAN –∫–ª–∞—Å—Ç–µ—Ä–∞–º
            scatter = plt.scatter(embeddings_2d[:, 0], embeddings_2d[:, 1],
                                  c=cluster_labels, cmap='tab20', alpha=0.7, s=30)

            # –û–±–≤–æ–¥–∏–º –ø—Ä–æ–±–ª–µ–º–Ω—ã–µ negative (high similarity)
            neg_mask = all_labels == 0
            if neg_mask.any():
                neg_points = embeddings_2d[neg_mask]
                plt.scatter(neg_points[:, 0], neg_points[:, 1],
                            facecolors='none', edgecolors='red', s=100,
                            linewidth=1.5, label='Negative –ø–∞—Ä—ã')

            plt.colorbar(scatter, label='DBSCAN Cluster')
            plt.xlabel('t-SNE 1')
            plt.ylabel('t-SNE 2')
            plt.title('DBSCAN –∫–ª–∞—Å—Ç–µ—Ä–∏–∑–∞—Ü–∏—è —ç–º–±–µ–¥–¥–∏–Ω–≥–æ–≤')
            plt.legend()
            plt.grid(True, alpha=0.3)
            plt.savefig('dbscan_clusters.png', dpi=150)
            plt.show()


    problem_indices, clusters_info, dbscan = find_problematic_negatives_with_dbscan(
        model, apt_neg_emb, smi_neg_emb, threshold=0.5, eps=0.4
    )



    if clusters_info:
        print("   1. –ö—Ä—É–ø–Ω—ã–µ –∫–ª–∞—Å—Ç–µ—Ä—ã (>10 –ø–∞—Ä) - –≤–æ–∑–º–æ–∂–Ω–æ, –æ—à–∏–±–∫–∏ —Ä–∞–∑–º–µ—Ç–∫–∏")
        print("   2. –ú–∞–ª–µ–Ω—å–∫–∏–µ –∫–ª–∞—Å—Ç–µ—Ä—ã (2-5 –ø–∞—Ä) - —Å–ª–æ–∂–Ω—ã–µ —Å–ª—É—á–∞–∏ –¥–ª—è –º–æ–¥–µ–ª–∏")
        print("   3. –í—ã–±—Ä–æ—Å—ã (noise) - –∞–Ω–æ–º–∞–ª–∏–∏, –≤–æ–∑–º–æ–∂–Ω–æ —à—É–º –≤ –¥–∞–Ω–Ω—ã—Ö")

